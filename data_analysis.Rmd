---
title: "Untitled"
output: html_document
---
# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# setting up environment
setwd("~/Yale/2022 Fall/Tobin RA")
# loading packages
library(readxl)
library(tidyverse)
library(lubridate)
library(Hmisc)
library(heplots)
library(ggExtra)

# loading data
load("alldata.RData")
load("allyears.RData")
dfList <- list(df_2015, df_2016, df_2017, df_2018, df_2019, df_2020)

```


# Checking Counts

## Checking Gender Counts
```{r}
### sanity check for gender counts ###
genderdist <- function(dat){
  hist1 <- ggplot(dat, aes(x=GENDER)) +
    geom_bar(stat="count", width=0.7, fill="steelblue")
  return(hist1)
}

# genderdist(df_2015)
# genderdist(df_2016)
# genderdist(df_2017)
# genderdist(df_2018)
# genderdist(df_2019)
# genderdist(df_2020)

for (i in 1:length(2015:2020)){
  iter <- filter(alldata, year(alldata$UPDATE_DATE) == c(2015:2020)[i])
  print(genderdist(iter))
}

```

There are a fairly even number of responses from both males and females across the years. It seems the proportion of "Other" resposnes has been slightly increasing. This could be due to better clarity for the option, shifts in social norms, or maybe better anti-discriminatory policy.

There are many NAs, we should discuss what to do with these later.

## Checking unique responses
```{r}
### response count stats
nrow(df_2015)
nrow(df_2016)
nrow(df_2017)
nrow(df_2018)
nrow(df_2019)
nrow(df_2020)
# number of responses changes a lot! Is this an issue?

### checking if there are duplicates within each year
length(unique(df_2015$PID))-nrow(df_2015)
length(unique(df_2016$PID))-nrow(df_2016)
length(unique(df_2017$PID))-nrow(df_2017)
length(unique(df_2018$PID))-nrow(df_2018)
length(unique(df_2019$PID))-nrow(df_2019)
length(unique(df_2020$PID))-nrow(df_2020)
# no duplicates within year
```

There are no duplicates within each year.

# Checking dates
```{r}
str(alldata$UPDATE_DATE) 
# already in a date format!!

# no NAs across all years (I deleted code for this)

# organizing dates by month
# df_2015$newdates <- floor_date(df_2015$UPDATE_DATE, unit = "month")
# df_2016$newdates <- floor_date(df_2016$UPDATE_DATE, unit = "month")
# df_2017$newdates <- floor_date(df_2017$UPDATE_DATE, unit = "month")
# df_2018$newdates <- floor_date(df_2018$UPDATE_DATE, unit = "month")
# df_2019$newdates <- floor_date(df_2019$UPDATE_DATE, unit = "month")
# df_2020$newdates <- floor_date(df_2020$UPDATE_DATE, unit = "month")


# seeing how dates are dispersed, checking for weird ones
date_hist <- function(x){
  hist1 <- ggplot(data = x, aes(x = floor_date(UPDATE_DATE, unit = "month"))) +
    geom_bar(stat = "count", fill="steelblue") 
  return(hist1)
}



for (i in 1:length(2015:2020)){
  iter <- filter(alldata, year(alldata$UPDATE_DATE) == c(2015:2020)[i])
  iterhist <- date_hist(iter) + 
    xlab(as.character(c(2015:2020)[i]))
  print(iterhist)
}
```

The first few years had some weird spreads of dates, I'll check these further individually.

```{r}
#table(df_2015$newdates)
```

The dataframe for 2016 had survey response dates goinog all the way to december of 2017. I'm guessing we should remove these?

```{r}
#table(df_2016$newdates)
```

The same is true for 2016.

```{r}
#table(df_2017$newdates)
```

2017 had 2 extraneous dates from 2016. Weird.

All in all, things seem to be mostly in order. 


# Checking Industry

```{r}
# dfList %>%
#     lapply(., function(x) sum(is.na(x$Industry))) %>%
#     unlist() /
#   (dfList %>%
#     lapply(., function(x) nrow(x)) %>%
#     unlist())

sum(is.na(alldata$Industry)) / nrow(alldata)
# about 12% missing data
```

About 12% of data for Industry is missing.

Let's see some breakdowns of these industries
```{r}
indhist <- function(x){
  hist1 <- ggplot(data=x, aes(x=Industry)) +
    geom_bar(stat="count", width=0.7, fill="steelblue")
  return(hist1)
}

sort(table(alldata$Industry))
sum(is.na(alldata$Industry)) / nrow(alldata)
```

This is kind of messy, I'm not sure if we want to really get into the weeds with this yet. 

# Checking yrs exp

```{r}
exphist <- function(x){
  hist1 <- ggplot(data=x, aes(x=YRS_EXP))+
    geom_density(stat="count", alpha=0.6, fill="steelblue")
}

for (i in 1:length(2015:2020)){
  iter <- filter(alldata, year(alldata$UPDATE_DATE) == c(2015:2020)[i])
  print(exphist(iter))
}
```

Overall the spreads of work experience are right skewed, which makes sense.

# Checking hourly_rate
```{r}
describe(alldata$HOURLY_RATE)
quantile(alldata$HOURLY_RATE, .99, na.rm=TRUE)
```
# 95th, 99th percentile makes sense, there are a few outliers that may be from very heavily salaried employees.

# checking TCC spread
```{r}
salaryhist <- function(x){
  hist1 <- ggplot(data=x, aes(x=TCC)) +
    geom_density(fill = 'steelblue', alpha=0.75) +
    xlim(c(0,401622))
}

#sample stats
describe(alldata$TCC)
quantile(alldata$TCC, 0.99, na.rm = TRUE)

for (i in 1:length(2015:2020)){
  iter <- filter(alldata, year(alldata$UPDATE_DATE) == c(2015:2020)[i])
  print(salaryhist(iter))
}
```

There's a ton of outliers! We should make a cutoff for this. I've set it to 401,622, the 99th percentile income in 2020.

# TCC by gender
```{r}
# trying to compare female to male
p1 <- ggplot(data = df_2015) +
        geom_boxplot(aes(x = GENDER, y = SALARY, col = GENDER),
                    width = 0.1) + # jittered
        theme(legend.position = c(0.1, .73), # adding legend
              plot.title = element_text(vjust = 0.25) # moving title
              ) +
        ylim(c(0, 250000)) +
  theme_bw()

# adding marginal histograms to scatterplot, not nec for box    
# p2 <- ggMarginal(p1, groupFill = TRUE, margins = "y") 

# overlaid histogram for salary vs gender
salaryhist2 <- function(x){
  x %>%
    filter(!is.na(GENDER)) %>%
  ggplot() +
    geom_density(aes(x = TCC, fill = GENDER), #TCC has much less NA than salary
                 alpha=0.25) + 
    theme(legend.position = c(0.1, .73), # adding legend
          plot.title = element_text(vjust = 0.25)) + # moving title
    xlim(c(0,200000)) +
    theme_minimal() +
    ylim(0, 3.5 * 10 ** (-5)) +
    scale_fill_manual(values = c("#FF0000", "#169B45", "#0F31D8"))
}

for (i in 1:length(2015:2020)){
  iter <- filter(alldata, year(alldata$UPDATE_DATE) == c(2015:2020)[i])
  print(salaryhist2(iter))
}

```

# Educ by gender
```{r}
table(alldata$EDU_LVL)

ggplot(alldata, aes(fill=GENDER, x=Industry)) + 
    geom_bar( stat="count")

alldata %>% group_by(Industry, GENDER) %>% summarise(n=n())
# better in console
```



```{r}
# chi-square quantile plot function, credit to JDRS
#A function to make chi-square quantile plots 
#to test for multivariate normality of data or residuals
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
  #usually, vars is xxx$residuals or data from one group and label is for plot
  x<-cov(scale(vars),use="pairwise.complete.obs")
  squares<-sort(diag(as.matrix(scale(vars))%*%solve(x)%*%as.matrix(t(scale(vars)))))
  quantiles<-quantile(squares)
  hspr<-quantiles[4]-quantiles[2]
  cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
  degf<-dim(x)[1]
  quants<-qchisq(cumprob,df=degf)
  gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
  scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
  se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
  lower<-quants-2*se
  upper<-quants+2*se
  
  plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
       ylab="Squared MH Distance",main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
  lines(c(0,100),c(0,100),col=1)
  lines(quants,upper,col="blue",lty=2,lwd=2)
  lines(quants,lower,col="blue",lty=2,lwd=2)
  legend("topleft",c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
         pch=c(19,NA))
}
#An example of usage of this function
#CSQPlot(danielaaov$residuals,label="Daniela MANOVA Residuals")
```

Note: tests for multivariate normality must have shorter tables, should I
just sample them?

```{r}

#CSQPlot(num2015) #error: not enough ram
#heplots::cqplot(num2015)
# library(QuantPsyc)
# mult.norm(num2015)$mult.test
# CSQPlot(sample_n(num2015,500))

cor(num2015$HOURLY_RATE, num2015$SALARY, use="complete.obs")
# interesting error - is it either salary or wage reported?
```


# Location State
```{r}
table(alldata$LOCATION_STATE)

```
A possible cool way to plot state data, probably not necessary
https://urban-institute.medium.com/how-to-create-state-and-county-maps-easily-in-r-577d29300bb2
